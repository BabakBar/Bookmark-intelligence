# Implementation Plan: Phase 0 Bootstrap - Import & Organize Existing Bookmarks

**Branch**: `002-bootstrap-import` | **Date**: October 27, 2025 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/002-bootstrap-import/spec.md`

**Note**: This plan is generated by the `/speckit.plan` command.

## Summary

Import and organize existing bookmark collections (800+ bookmarks) from browser HTML exports, process them with AI to generate embeddings/tags/summaries, automatically cluster them into semantic groups, suggest initial projects, and provide a web interface for validation. This is the critical first step (Phase 0) that must complete before building the browser extension (Phase 1).

**Primary Requirement**: Transform chaotic, unorganized bookmark exports into a structured, AI-organized, searchable collection with zero data loss.

**Technical Approach**:
- HTML parsing using `bookmarks-parser` Python library
- Batch AI processing with OpenAI Batch API (50% cost savings) and Claude 3.5 Haiku
- MiniBatchKMeans clustering with cosine similarity and elbow method for optimal K
- FastAPI backend + React web dashboard for validation
- Self-hosted on Hetzner VPS via Coolify

## Technical Context

**Language/Version**: Python 3.13+ (backend), TypeScript 5.x (frontend web dashboard)
**Primary Dependencies**:
- Backend: FastAPI 0.115+, SQLAlchemy 2.0+, Qdrant Python SDK, OpenAI SDK, Anthropic SDK, bookmarks-parser, scikit-learn (clustering), Celery 5.4+
- Frontend: React 18+, Vite 5+, Tailwind CSS 3+, Axios

**Storage**:
- PostgreSQL 17 (bookmark metadata, users, projects, clusters)
- Qdrant 1.12 (embedding vectors for semantic search)
- Valkey 7.4 (Celery task queue, caching)

**Testing**: pytest (backend unit/integration), Vitest (frontend), contract tests for API endpoints

**Target Platform**:
- Backend: Linux server (Ubuntu 24.04) on Hetzner VPS CX32
- Frontend: Web browsers (Chrome, Firefox, Safari)
- Deployment: Docker containers via Coolify

**Project Type**: Web application (backend API + frontend web dashboard)

**Performance Goals**:
- Import: Parse 1000 bookmarks in <30 seconds
- AI Processing: Complete 800 bookmarks in <24 hours
- Search: <500ms for keyword search, <2s for semantic search
- Web UI: <2s page load for 1000 bookmarks

**Constraints**:
- Zero data loss during import (100% success rate)
- AI cost: <$0.06 per bookmark ($40-60 total for 800 bookmarks)
- Processing time: <24 hours for batch AI operations
- Infrastructure: Must fit on 8GB RAM VPS (PostgreSQL 1.5GB, Qdrant 2GB, Valkey 512MB, FastAPI 1GB, Celery 1GB)

**Scale/Scope**:
- Initial: 1 user (Sia), 800 bookmarks
- Target: Support 100-200 users with 200-1000 bookmarks each
- Clustering: 8-15 clusters per user
- Projects: 3-5 suggested projects per user

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

**Note**: Project constitution file exists at `.specify/memory/constitution.md` but is currently a template. No specific gates to check at this time. As the constitution is defined, this section will be updated to verify:
- Test-first development compliance
- Library-first architecture (if applicable)
- Observability requirements
- Versioning standards

**Current Status**: ✅ No violations (constitution template not yet filled)

## Project Structure

### Documentation (this feature)

```text
specs/002-bootstrap-import/
├── spec.md              # Feature specification (DONE)
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (NEXT - AI/clustering/parsing research)
├── data-model.md        # Phase 1 output (database schemas, entities)
├── quickstart.md        # Phase 1 output (setup instructions)
├── contracts/           # Phase 1 output (API OpenAPI specs)
│   ├── import.yaml      # Import endpoints
│   ├── bookmarks.yaml   # Bookmark CRUD
│   ├── clusters.yaml    # Clustering endpoints
│   └── projects.yaml    # Project suggestions
├── checklists/
│   └── requirements.md  # Quality checklist (DONE)
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created yet)
```

### Source Code (repository root)

```text
# Web application structure (backend + frontend)

backend/
├── app/
│   ├── main.py                      # FastAPI application entry
│   ├── api/
│   │   └── v1/
│   │       ├── endpoints/
│   │       │   ├── import.py        # Import bookmark HTML files
│   │       │   ├── bookmarks.py     # Bookmark CRUD (basic for Phase 0)
│   │       │   ├── clusters.py      # Clustering operations
│   │       │   ├── projects.py      # Project suggestions
│   │       │   └── auth.py          # User authentication
│   │       └── deps.py              # Dependencies (DB session, auth)
│   ├── core/
│   │   ├── config.py                # Settings (Pydantic BaseSettings)
│   │   ├── security.py              # JWT auth utilities
│   │   └── database.py              # SQLAlchemy async setup
│   ├── models/
│   │   ├── user.py                  # User SQLAlchemy model
│   │   ├── bookmark.py              # Bookmark model
│   │   ├── project.py               # Project model
│   │   ├── cluster.py               # Cluster model
│   │   └── import_job.py            # Import job tracking
│   ├── schemas/
│   │   ├── bookmark.py              # Pydantic schemas (request/response)
│   │   ├── project.py
│   │   ├── cluster.py
│   │   └── import_job.py
│   ├── services/
│   │   ├── html_parser.py           # HTML bookmark parsing (bookmarks-parser)
│   │   ├── embedding_service.py     # OpenAI embedding generation
│   │   ├── ai_service.py            # Claude API for tags/summaries
│   │   ├── clustering_service.py    # MiniBatchKMeans clustering
│   │   ├── project_service.py       # Project suggestion algorithm
│   │   └── vector_store.py          # Qdrant operations
│   ├── tasks/
│   │   ├── celery_app.py            # Celery configuration
│   │   ├── import_tasks.py          # Batch import processing
│   │   ├── ai_tasks.py              # Batch AI tagging/summarization
│   │   └── clustering_tasks.py      # Batch clustering
│   └── db/
│       └── migrations/              # Alembic migration scripts
├── tests/
│   ├── contract/                    # API contract tests
│   ├── integration/                 # Integration tests (DB, AI APIs)
│   └── unit/                        # Unit tests for services
├── requirements.txt
├── Dockerfile
└── .env.example

frontend/
├── src/
│   ├── components/
│   │   ├── ImportUploader.tsx       # File upload component
│   │   ├── BookmarkGrid.tsx         # Bookmark display grid
│   │   ├── ClusterView.tsx          # Cluster visualization
│   │   ├── ProjectCard.tsx          # Project suggestion card
│   │   └── ProgressBar.tsx          # Import/processing progress
│   ├── pages/
│   │   ├── Dashboard.tsx            # Main dashboard (stats, summary)
│   │   ├── Import.tsx               # Import page
│   │   ├── Bookmarks.tsx            # Browse bookmarks
│   │   ├── Clusters.tsx             # Manage clusters
│   │   └── Projects.tsx             # Review project suggestions
│   ├── services/
│   │   ├── api.ts                   # Axios API client
│   │   └── auth.ts                  # Auth helpers
│   ├── hooks/
│   │   ├── useBookmarks.ts          # React Query hooks
│   │   ├── useClusters.ts
│   │   └── useProjects.ts
│   ├── App.tsx
│   └── main.tsx
├── tests/
│   └── unit/                        # Component tests
├── package.json
├── vite.config.ts
├── tailwind.config.js
└── Dockerfile

# Deployment configuration
coolify/
├── docker-compose.yml               # All services (PostgreSQL, Qdrant, Valkey, backend, frontend, Celery)
└── .env.production                  # Production environment variables
```

**Structure Decision**:
- **Web application** structure chosen because this phase requires both a backend API (for import processing, AI operations, database) and a frontend web dashboard (for validation and user interaction)
- Backend handles all heavy lifting (HTML parsing, AI processing, clustering)
- Frontend is a simple validation interface (not the full browser extension - that comes in Phase 1)
- Separated frontend/backend allows independent scaling and deployment
- Celery workers run as separate containers for background batch processing

## Complexity Tracking

> No constitution violations at this time. Constitution template not yet defined.

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| N/A | N/A | N/A |

---

## Next Steps

**Phase 0 (Research)**: Generate `research.md` with:
1. HTML bookmark parsing library comparison (bookmarks-parser vs alternatives)
2. Batch AI processing best practices (OpenAI Batch API vs real-time)
3. Clustering algorithm evaluation (MiniBatchKMeans vs KMeans vs HDBSCAN)
4. Vector database integration patterns (Qdrant Python SDK usage)
5. Web framework choices for validation dashboard (React vs Vue vs Svelte)

**Phase 1 (Design)**: Generate:
- `data-model.md`: Database schemas for users, bookmarks, projects, clusters, import jobs
- `contracts/`: OpenAPI specs for all API endpoints
- `quickstart.md`: Local development setup instructions
